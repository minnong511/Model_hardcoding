{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper :  https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/img_1.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images\\img_2.png\" width = 800 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention Function \n",
    "    - Query : 어떤 정보가 중요한가에 대한 질문 ? \n",
    "    - Key : Value에서 Query에 필요한 부분은 어디인가? \n",
    "    - Value  : 최종적으로 선택될 정보 (출력과 연관)\n",
    "\n",
    "input(Q,K,V) 를 통해서 output Vector를 생성 \n",
    "물론 Q,K,V  모두 Vector 이다. \n",
    "\n",
    "\n",
    "- Weighted Sum \n",
    "\n",
    "최종 출력은 입력 값들로 결정된다. \n",
    "Value는 Query 와 Key의 유사도를 기반으로 가중치가 배치됨. \n",
    "최종적으로는 Softmax 함수를 지나면서 확률분포로 변환된다. \n",
    "\n",
    "\n",
    "- 결론 \n",
    "    - Q,K,V를 기반으로 유사도를 계산하고, 가중치를 적용하여 최종 출력을 만든다. \n",
    "    - 출력은 Value(V) 와 Weighted Sum으로 결정한다. \n",
    "    - 이와 같이 연산하면, 입력 데이터 중에 필요한 부분 집중이 가능. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product \n",
    "\n",
    "- The inputs consists of (Q)queries and (K)keys of dimension $d_{k}$\n",
    "- and (V)Values of dimension $d_{v}$\n",
    "\n",
    "### To briefly summarize the inputs \n",
    "\n",
    "$$\n",
    "Q,K = d_{k} \\\\ \n",
    "V = d_{v}\n",
    "$$ \n",
    "\n",
    "\n",
    "- In this process, we need to perform some calculations\n",
    "1. the dot products of the (Q)query with all (K)keys ,\n",
    "2. divide each by $\\sqrt{d_{k}}$ \n",
    "3. apply softmax function to obtain the weights on the values \n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}}) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention \n",
    "\n",
    "- In this paper, the authors found that it is beneficial to linearly project the queries, keys and values h times with different, learned linear projections to $d_{k}$\n",
    "- On each of these projected versions of queries, keys and values are processed in parallel\n",
    "- These are concatenated and once again projected, resulting in the fianl values \n",
    "\n",
    "### formula  \n",
    "\n",
    "$$\n",
    "\\mathrm{MultuHead}(Q,K,V)  = \\mathrm{Concat}(head_{1},.....,head{h})W^{O} \\\\ \n",
    " \\mathrm{where} \\  \\mathrm{head}_{i} = \\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})\n",
    "$$ \n",
    "\n",
    "The key point is that each attention head applies diffrent learned Weighted sum before computing the weighted sum   \n",
    "\n",
    "where the projections are paraemeter matrices \n",
    "\n",
    "$W^{Q}_{i} \\ \\in \\ \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\ W^{K}_{i} \\ \\in \\ \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\ W^{V}_{i} \\ \\in \\ \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "\n",
    "- In this paper, the authors employ h = 8 parrallel attention layers, or heads. \n",
    "- For each of these, the model uses $d_{k} = d_{v} = d_{model}/h = 64$\n",
    "\n",
    "- By reducing the dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality\n",
    "\n",
    "### Summary \n",
    "\n",
    "- Multi-head Attention allows the model to jointly attend(집중하도록록) to information from different representation subspaces at different positions.\n",
    "- With a Single-head Attention, averaging inhibits this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Attention in model\n",
    "\n",
    "- The Transformer uses multi-head attention in three different ways\n",
    "    - \"Encoder - Decoder Attention\" Layer\n",
    "        - Queries come from the previous layer \n",
    "        - Keys and values come form Encoder's output \n",
    "        - This allows every position in the decoder to attend over all positions in the sequence \n",
    "    - \"Encoder - Self Attention\" Layer\n",
    "    - \"Decoder - Self Attention\" Layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Minnong_pytorch_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
