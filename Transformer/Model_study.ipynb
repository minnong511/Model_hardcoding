{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper :  https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/img_1.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images\\img_2.png\" width = 800 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention Function \n",
    "    - Query : 어떤 정보가 중요한가에 대한 질문 ? \n",
    "    - Key : Value에서 Query에 필요한 부분은 어디인가? \n",
    "    - Value  : 최종적으로 선택될 정보 (출력과 연관)\n",
    "\n",
    "input(Q,K,V) 를 통해서 output Vector를 생성 \n",
    "물론 Q,K,V  모두 Vector 이다. \n",
    "\n",
    "\n",
    "- Weighted Sum \n",
    "\n",
    "최종 출력은 입력 값들로 결정된다. \n",
    "Value는 Query 와 Key의 유사도를 기반으로 가중치가 배치됨. \n",
    "최종적으로는 Softmax 함수를 지나면서 확률분포로 변환된다. \n",
    "\n",
    "\n",
    "- 결론 \n",
    "    - Q,K,V를 기반으로 유사도를 계산하고, 가중치를 적용하여 최종 출력을 만든다. \n",
    "    - 출력은 Value(V) 와 Weighted Sum으로 결정한다. \n",
    "    - 이와 같이 연산하면, 입력 데이터 중에 필요한 부분 집중이 가능. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product \n",
    "\n",
    "- The inputs consists of (Q)queries and (K)keys of dimension $d_{k}$\n",
    "- and (V)Values of dimension $d_{v}$\n",
    "\n",
    "### To briefly summarize the inputs \n",
    "\n",
    "$$\n",
    "Q,K = d_{k} \\\\ \n",
    "V = d_{v}\n",
    "$$ \n",
    "\n",
    "\n",
    "- In this process, we need to perform some calculations\n",
    "1. the dot products of the (Q)query with all (K)keys ,\n",
    "2. divide each by $\\sqrt{d_{k}}$ \n",
    "3. apply softmax function to obtain the weights on the values \n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}}) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention \n",
    "\n",
    "- In this paper, the authors found that it is beneficial to linearly project the queries, keys and values h times with different, learned linear projections to $d_{k}$\n",
    "- On each of these projected versions of queries, keys and values are processed in parallel\n",
    "- These are concatenated and once again projected, resulting in the fianl values \n",
    "\n",
    "### formula  \n",
    "\n",
    "$$\n",
    "\\mathrm{MultuHead}(Q,K,V)  = \\mathrm{Concat}(head_{1},.....,head{h})W^{O} \\\\ \n",
    " \\mathrm{where} \\  \\mathrm{head}_{i} = \\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})\n",
    "$$ \n",
    "\n",
    "The key point is that each attention head applies diffrent learned Weighted sum before computing the weighted sum   \n",
    "\n",
    "where the projections are paraemeter matrices \n",
    "\n",
    "$W^{Q}_{i} \\ \\in \\ \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\ W^{K}_{i} \\ \\in \\ \\mathbb{R}^{d_{\\text{model}} \\times d_k}, \\ W^{V}_{i} \\ \\in \\ \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
    "\n",
    "- In this paper, the authors employ h = 8 parrallel attention layers, or heads. \n",
    "- For each of these, the model uses $d_{k} = d_{v} = d_{model}/h = 64$\n",
    "\n",
    "- By reducing the dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality\n",
    "\n",
    "### Summary \n",
    "\n",
    "- Multi-head Attention allows the model to jointly attend(집중하도록록) to information from different representation subspaces at different positions.\n",
    "- With a Single-head Attention, averaging inhibits this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Attention in model\n",
    "\n",
    "- The Transformer uses multi-head attention in three different ways\n",
    "    - \"Encoder - Decoder Attention\" Layer\n",
    "        - Queries come from the previous layer \n",
    "        - Keys and values come form Encoder's output \n",
    "        - This allows every position in the decoder to attend over all positions in the sequence \n",
    "    - \"Encoder - Self Attention\" Layer\n",
    "        - Keys, Values and queries are come from the same place \n",
    "            - In this case, the output of the previous layer in the encoder \n",
    "        - Each position in the encoder can attend to all positions in the previous layer of the encoder  \n",
    "    - \"Decoder - Self Attention\" Layer\n",
    "        - this allows each positions in the decoder to attend to all positions in the decoder up to including that positons \n",
    "        (디코더가 그 위치까지의 모든 단어를 참조할 수 있게 만들어 준다. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks \n",
    "\n",
    "- Including Sublayers, each of layers in the encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically \n",
    "\n",
    "### FFN (Feed-Forward-Network) \n",
    "\n",
    "$$\n",
    "\\mathrm{FFN}(x) = \\mathrm{max}(0,xW_{1} + b_{1})W_{2} + b_{2}  \n",
    "$$\n",
    "\n",
    "- It consists of two linear transformations with a ReLU activation in between \n",
    "- (용어 설명 : 선형변환은 행렬곱과 Bias 합을 포함한다)\n",
    "\n",
    "### How can the linear combinations applies to the layers? \n",
    "\n",
    "- the linear transformations are the same across different positions , they use different parameters from layer to layer \n",
    "    - 좀 더 쉽게 설명하자면, 커널 사이즈 1인 CNN - 1D convolution을 생각하면 편하다. \n",
    "    - 각 단어 위ㅊ에 독립적으로 같은 변환이 이루어진다. \n",
    "\n",
    "\n",
    "### The dimensionality of input and output \n",
    "- input and output dimension : $d_{model} = 512$\n",
    "- inner - layer dimension $d_{ff} = 2048$ (ff는 FFN의 약자)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여기서 잠깐 짚고 넣어가는 RNN vs LSTM vs Transformer \n",
    "\n",
    "- RNN : 순차적으로 데이터 처리, 가벼운 연산량. 그러나 장기 의존성(Grdient Vanishing) 및 병렬 연산 불가능\n",
    "\n",
    "- LSTM : RNN의 단점은 극복했지만, 병렬 연산이 불가능하여 빠른 학습이 안됨 \n",
    "\n",
    "- Transformer : RNN,LSTM의 순차적 처리 문제를 해결, 심지어 Self-attention으로 한 번에 전체 문맥을 학습한다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Softmax  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Minnong_pytorch_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
